# 4 Methods that Power Feature Selection in a Machine Learning Model
 
**Feature Selection** in Machine Learning is selecting the most impactful features, or columns, in a dataset. Does your dataset have many columns, and do you want to see which have the biggest impact? Do you want to discard those that aren't generating much value? By performing feature selection, you're not only **reducing the amount of data** that needs to be processed to speed up your analysis, but you're **simplifying the interpretation** of the model, making it easier to understand.

Depending on the types of data you have, one can use several techniques, ranging from Statistical methods to leveraging a machine learning model to make the selection. We'll look at a few of the most common techniques and see how they are applied in practice!

1. Categorical data using the **Chi-Squared Test**
2. **Pearson's Correlation Coefficient** for Numeric Data
3. **Principal Component Analysis** for Numeric Data
4. **Feature Importance** with **Random Forests** for Both Categorical and Numeric Data

Read more here: https://www.dataknowsall.com/featureselection.html
